
from langchain.vectorstores import InMemoryVectorStore
from langchain_ollama import OllamaEmbeddings

def rag_basic(chunks: list[str], question: str, embeddings, llm, k=3) -> list[str]:
    """
    Perform a basic RAG (Retrieval-Augmented Generation) process.
    This function retrieves relevant documents based on the provided question and
    generates a response using the specified language model.
    Args:
        chunks (list[str]): List of text chunks to search for relevant information.
        question (str): The user's query or question.
        embeddings: The embedding model to use for document retrieval.
        llm: The language model to use for generating the final answer.
    Returns:
        list[str]: The final answer generated by the language model.
    """


    embeddings = OllamaEmbeddings(
        model="llama3",
    )

    vectorstore = InMemoryVectorStore.from_texts(
            chunks,
            embedding=embeddings,
        )

    retriever = vectorstore.as_retriever(k=3)
    retrieved_documents = retriever.invoke(question)
    retrieved_documents = [doc.page_content for doc in retrieved_documents]

    return retrieved_documents